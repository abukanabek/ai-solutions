{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13713624,"sourceType":"datasetVersion","datasetId":8691299}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:16.595679Z","iopub.execute_input":"2025-11-13T06:28:16.595953Z","iopub.status.idle":"2025-11-13T06:28:20.959504Z","shell.execute_reply.started":"2025-11-13T06:28:16.595933Z","shell.execute_reply":"2025-11-13T06:28:20.958872Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:20.960757Z","iopub.execute_input":"2025-11-13T06:28:20.961595Z","iopub.status.idle":"2025-11-13T06:28:21.028852Z","shell.execute_reply.started":"2025-11-13T06:28:20.961548Z","shell.execute_reply":"2025-11-13T06:28:21.028023Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"class2idx = {\n    'no_relevant': 0,\n    'relevant_minus': 1,\n    'relevant': 2,\n    'relevant_plus': 3,\n}\n\nidx2class = {v: k for k, v in class2idx.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:21.029922Z","iopub.execute_input":"2025-11-13T06:28:21.030202Z","iopub.status.idle":"2025-11-13T06:28:21.042057Z","shell.execute_reply.started":"2025-11-13T06:28:21.030173Z","shell.execute_reply":"2025-11-13T06:28:21.041425Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# posts = pd.read_parquet(\"/kaggle/input/russian-ai-olympiad-2024-b/items.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:21.043528Z","iopub.execute_input":"2025-11-13T06:28:21.043787Z","iopub.status.idle":"2025-11-13T06:28:21.057312Z","shell.execute_reply.started":"2025-11-13T06:28:21.043769Z","shell.execute_reply":"2025-11-13T06:28:21.056696Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# itemId2idx = dict()\n# for i, row in tqdm(posts.iterrows(), total=len(posts)):\n#     itemId2idx[row['itemId']] = i","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:21.058032Z","iopub.execute_input":"2025-11-13T06:28:21.058243Z","iopub.status.idle":"2025-11-13T06:28:21.072026Z","shell.execute_reply.started":"2025-11-13T06:28:21.058219Z","shell.execute_reply":"2025-11-13T06:28:21.071460Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# train = pd.read_csv(\"/kaggle/input/russian-ai-olympiad-2024-b/train.csv\")\n# test = pd.read_csv(\"/kaggle/input/russian-ai-olympiad-2024-b/test.csv\")\n# subm = pd.read_csv(\"/kaggle/input/russian-ai-olympiad-2024-b/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:21.072740Z","iopub.execute_input":"2025-11-13T06:28:21.073182Z","iopub.status.idle":"2025-11-13T06:28:21.084931Z","shell.execute_reply.started":"2025-11-13T06:28:21.073155Z","shell.execute_reply":"2025-11-13T06:28:21.084212Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# train = train[~train['target'].isin(['doubles', 'unavailable'])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:21.085615Z","iopub.execute_input":"2025-11-13T06:28:21.085787Z","iopub.status.idle":"2025-11-13T06:28:21.101377Z","shell.execute_reply.started":"2025-11-13T06:28:21.085772Z","shell.execute_reply":"2025-11-13T06:28:21.100739Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# tqdm.pandas()\n\n# def get_data_from_posts(row):\n#     idx1, idx2 = itemId2idx[row['leftItemId']], itemId2idx[row['rightItemId']]\n#     post1, post2 = posts.iloc[idx1], posts.iloc[idx2]\n\n#     if row.get('target') is not None:\n#         row['target'] = class2idx[row['target']]\n    \n#     row['text1'] = post1['title']\n    \n#     row['text2'] = post2['title']\n\n#     row['same_author'] = 1 if post1['authorId'] == post2['authorId'] else 0\n    \n#     return row\n    \n\n# train = train.progress_apply(get_data_from_posts, axis=1)\n# train.drop(columns=['leftItemId', 'rightItemId'], inplace=True)\n\n# test = test.progress_apply(get_data_from_posts, axis=1)\n# test.drop(columns=['leftItemId', 'rightItemId'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:21.102162Z","iopub.execute_input":"2025-11-13T06:28:21.102386Z","iopub.status.idle":"2025-11-13T06:28:21.116181Z","shell.execute_reply.started":"2025-11-13T06:28:21.102359Z","shell.execute_reply":"2025-11-13T06:28:21.115514Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# train.to_parquet(\"new_train.parquet\")\n# test.to_parquet(\"new_test.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:21.116928Z","iopub.execute_input":"2025-11-13T06:28:21.117164Z","iopub.status.idle":"2025-11-13T06:28:21.128819Z","shell.execute_reply.started":"2025-11-13T06:28:21.117137Z","shell.execute_reply":"2025-11-13T06:28:21.128200Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train = pd.read_parquet(\"/kaggle/input/russian-ai-olympiad-2024-b/new_train.parquet\")\ntest = pd.read_parquet(\"/kaggle/input/russian-ai-olympiad-2024-b/new_test.parquet\")\nsubm = pd.read_csv(\"/kaggle/input/russian-ai-olympiad-2024-b/sample_submission.csv\")\n\ntrain['text'] = train['text1'] + train['text2']\ntest['text'] = test['text1'] + test['text2']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:21.131657Z","iopub.execute_input":"2025-11-13T06:28:21.131895Z","iopub.status.idle":"2025-11-13T06:28:22.725324Z","shell.execute_reply.started":"2025-11-13T06:28:21.131857Z","shell.execute_reply":"2025-11-13T06:28:22.724481Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# train, _ = train_test_split(train, test_size=0.6, stratify=train['target'], random_state=42)\n\ntrain, valid = train_test_split(train, test_size=0.05, stratify=train['target'], random_state=42)\n\ntrain.reset_index(drop=True, inplace=True)\nvalid.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:22.726283Z","iopub.execute_input":"2025-11-13T06:28:22.726652Z","iopub.status.idle":"2025-11-13T06:28:22.872324Z","shell.execute_reply.started":"2025-11-13T06:28:22.726629Z","shell.execute_reply":"2025-11-13T06:28:22.871742Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pip install --upgrade protobuf==3.20.3 -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:22.873043Z","iopub.execute_input":"2025-11-13T06:28:22.873266Z","iopub.status.idle":"2025-11-13T06:28:27.580445Z","shell.execute_reply.started":"2025-11-13T06:28:22.873248Z","shell.execute_reply":"2025-11-13T06:28:27.579702Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_checkpoint = \"DeepPavlov/rubert-base-cased-sentence\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=4).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:27.581460Z","iopub.execute_input":"2025-11-13T06:28:27.581720Z","iopub.status.idle":"2025-11-13T06:28:59.976481Z","shell.execute_reply.started":"2025-11-13T06:28:27.581697Z","shell.execute_reply":"2025-11-13T06:28:59.975786Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3131d7ebcb004f1ab27d138f1bf8d395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1240c141a274438bd3904f2e0ff2f54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94bc3606cdff43e5a88d34016b69243a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54ca7cffc91f4b9d882bb521b5282f3c"}},"metadata":{}},{"name":"stderr","text":"2025-11-13 06:28:38.188598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763015318.365443      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763015318.410215      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/711M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"206b9a1e97ce457d81f64aec9a4fdf7d"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%time\n\nsentences = [\"Hello World\", \"Привет Мир\"]\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=64, return_tensors='pt')\nencoded_input = {k: v.to(device) for k, v in encoded_input.items()}\nwith torch.no_grad():\n    logits = model(**encoded_input).logits\n\nprobs = torch.softmax(logits, dim=-1)\npreds = torch.argmax(probs, dim=-1)\npreds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:28:59.977326Z","iopub.execute_input":"2025-11-13T06:28:59.977879Z","iopub.status.idle":"2025-11-13T06:29:00.294654Z","shell.execute_reply.started":"2025-11-13T06:28:59.977848Z","shell.execute_reply":"2025-11-13T06:29:00.293816Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a2bc976e30c483b95aed9d0a6842806"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 112 ms, sys: 60 ms, total: 172 ms\nWall time: 307 ms\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([3, 0], device='cuda:0')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"model.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:29:00.295414Z","iopub.execute_input":"2025-11-13T06:29:00.295756Z","iopub.status.idle":"2025-11-13T06:29:00.300253Z","shell.execute_reply.started":"2025-11-13T06:29:00.295737Z","shell.execute_reply":"2025-11-13T06:29:00.299717Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"class_counts = torch.tensor(train['target'].value_counts().sort_index().values, dtype=torch.float)\nweights = 1.0 / class_counts  \n\nweights = weights / weights.sum() * len(class_counts)  \nweights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:29:00.300868Z","iopub.execute_input":"2025-11-13T06:29:00.301080Z","iopub.status.idle":"2025-11-13T06:29:00.334130Z","shell.execute_reply.started":"2025-11-13T06:29:00.301065Z","shell.execute_reply":"2025-11-13T06:29:00.333422Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor([0.9734, 0.5328, 0.5495, 1.9443])"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DataCollatorWithPadding\n\nBATCH_SIZE = 64\n\ndata_collator = DataCollatorWithPadding(tokenizer)\n\nclass PostDataset(Dataset):\n    def __init__(self, text, target=None):\n        self.text = text\n        self.encodings = tokenizer(self.text, padding=False, truncation=True, max_length=128, return_tensors=None)\n        self.target = target\n    \n    def __len__(self):\n        return len(self.text)\n    def __getitem__(self, idx):\n        enc = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n        if self.target is not None:\n            enc['labels'] = torch.tensor(self.target[idx], dtype=torch.long)\n        return enc\n\ntrain_ds = PostDataset(train['text'].tolist(), train['target'].tolist())\nvalid_ds = PostDataset(valid['text'].tolist(), valid['target'].tolist())\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=data_collator)\nvalid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, collate_fn=data_collator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:35:24.181002Z","iopub.execute_input":"2025-11-13T06:35:24.181644Z","iopub.status.idle":"2025-11-13T06:35:41.101253Z","shell.execute_reply.started":"2025-11-13T06:35:24.181593Z","shell.execute_reply":"2025-11-13T06:35:41.100469Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"epochs = 1\nlog_rate = 1\nbatch_log_rate = 50\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=4).to(device)\nmodel.to(device)\n\nloss_fn = nn.CrossEntropyLoss(weight=weights.to(device))\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-4)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:35:41.102699Z","iopub.execute_input":"2025-11-13T06:35:41.102977Z","iopub.status.idle":"2025-11-13T06:35:42.363128Z","shell.execute_reply.started":"2025-11-13T06:35:41.102953Z","shell.execute_reply":"2025-11-13T06:35:42.362311Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"for epoch in tqdm(range(epochs), desc='Training'):\n    model.train()\n    running_train_loss = 0\n    all_preds = []\n    i = 0\n    for X in (pbar := tqdm(train_loader, leave=False, desc='Train DataLoader')):\n        i += 1\n        y = X['labels'].to(device)\n        X = {k: v.to(device) for k, v in X.items() if k != 'labels'}\n        logits = model(**X).logits\n        loss = loss_fn(logits, y)\n\n        probs = torch.argmax(torch.softmax(logits, dim=-1), dim=-1)\n        all_preds.extend(probs.detach().cpu().tolist())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # scheduler.step()\n\n        running_train_loss += loss.item()\n        if i%batch_log_rate==0 or i==len(train_loader):\n            print(f\"Batch {i}/{len(train_loader)} | Train Loss: {running_train_loss/i:.5f}\")\n            # print(f\"Diversity: {torch.bincount(torch.tensor(all_preds)).tolist()}\")\n        pbar.set_postfix({'loss': f\"{running_train_loss/i:.5f}\",}) \n                          # \"diversity\": torch.bincount(torch.tensor(all_preds)).tolist()})\n        \n    if (epoch+1)%log_rate==0:\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {running_train_loss/i:.5f}\")\n\n    model.eval()\n    all_preds, all_targets = [], []\n    i = 0\n    for X in (pbar := tqdm(valid_loader, leave=False, desc='Valid DataLoader')):\n        i += 1\n        y = X['labels'].to(device)\n        X = {k: v.to(device) for k, v in X.items() if k != 'labels'}\n        with torch.no_grad():\n            logits = model(**X).logits\n        probs = torch.argmax(torch.softmax(logits, dim=-1), dim=-1)\n        all_preds.extend(probs.detach().cpu().tolist())\n        all_targets.extend(y.cpu().tolist())\n    \n    f1 = f1_score(all_targets, all_preds, average='weighted')\n    if (epoch+1)%log_rate==0:\n        print(f\"Weighted F1 Score: {f1:.5f}\")\n        print(f\"Diversity: {torch.bincount(torch.tensor(all_preds)).tolist()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:04:24.547515Z","iopub.execute_input":"2025-11-13T07:04:24.547820Z","iopub.status.idle":"2025-11-13T07:24:46.934271Z","shell.execute_reply.started":"2025-11-13T07:04:24.547797Z","shell.execute_reply":"2025-11-13T07:24:46.933299Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c753e199c2a244f292568e09a2a47df8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Train DataLoader:   0%|          | 0/3095 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Batch 50/3095 | Train Loss: 1.06428\nBatch 100/3095 | Train Loss: 1.05440\nBatch 150/3095 | Train Loss: 1.04107\nBatch 200/3095 | Train Loss: 1.03342\nBatch 250/3095 | Train Loss: 1.02329\nBatch 300/3095 | Train Loss: 1.01627\nBatch 350/3095 | Train Loss: 1.00681\nBatch 400/3095 | Train Loss: 1.00745\nBatch 450/3095 | Train Loss: 1.00653\nBatch 500/3095 | Train Loss: 1.00247\nBatch 550/3095 | Train Loss: 1.00082\nBatch 600/3095 | Train Loss: 1.00022\nBatch 650/3095 | Train Loss: 0.99847\nBatch 700/3095 | Train Loss: 0.99499\nBatch 750/3095 | Train Loss: 0.99383\nBatch 800/3095 | Train Loss: 0.99374\nBatch 850/3095 | Train Loss: 0.99314\nBatch 900/3095 | Train Loss: 0.99254\nBatch 950/3095 | Train Loss: 0.99212\nBatch 1000/3095 | Train Loss: 0.99007\nBatch 1050/3095 | Train Loss: 0.98900\nBatch 1100/3095 | Train Loss: 0.98740\nBatch 1150/3095 | Train Loss: 0.98440\nBatch 1200/3095 | Train Loss: 0.98270\nBatch 1250/3095 | Train Loss: 0.98085\nBatch 1300/3095 | Train Loss: 0.98088\nBatch 1350/3095 | Train Loss: 0.97930\nBatch 1400/3095 | Train Loss: 0.97792\nBatch 1450/3095 | Train Loss: 0.97514\nBatch 1500/3095 | Train Loss: 0.97439\nBatch 1550/3095 | Train Loss: 0.97332\nBatch 1600/3095 | Train Loss: 0.97363\nBatch 1650/3095 | Train Loss: 0.97236\nBatch 1700/3095 | Train Loss: 0.97170\nBatch 1750/3095 | Train Loss: 0.96973\nBatch 1800/3095 | Train Loss: 0.96822\nBatch 1850/3095 | Train Loss: 0.96697\nBatch 1900/3095 | Train Loss: 0.96625\nBatch 1950/3095 | Train Loss: 0.96569\nBatch 2000/3095 | Train Loss: 0.96428\nBatch 2050/3095 | Train Loss: 0.96364\nBatch 2100/3095 | Train Loss: 0.96308\nBatch 2150/3095 | Train Loss: 0.96311\nBatch 2200/3095 | Train Loss: 0.96280\nBatch 2250/3095 | Train Loss: 0.96167\nBatch 2300/3095 | Train Loss: 0.96123\nBatch 2350/3095 | Train Loss: 0.96068\nBatch 2400/3095 | Train Loss: 0.96060\nBatch 2450/3095 | Train Loss: 0.95968\nBatch 2500/3095 | Train Loss: 0.95945\nBatch 2550/3095 | Train Loss: 0.95928\nBatch 2600/3095 | Train Loss: 0.95883\nBatch 2650/3095 | Train Loss: 0.95867\nBatch 2700/3095 | Train Loss: 0.95774\nBatch 2750/3095 | Train Loss: 0.95857\nBatch 2800/3095 | Train Loss: 0.95841\nBatch 2850/3095 | Train Loss: 0.95792\nBatch 2900/3095 | Train Loss: 0.95736\nBatch 2950/3095 | Train Loss: 0.95693\nBatch 3000/3095 | Train Loss: 0.95638\nBatch 3050/3095 | Train Loss: 0.95629\nBatch 3095/3095 | Train Loss: 0.95607\nEpoch 1/1 | Train Loss: 0.95607\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Valid DataLoader:   0%|          | 0/163 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Weighted F1 Score: 0.51900\nDiversity: [1856, 3856, 3345, 1368]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"test_ds = PostDataset(test['text'].tolist())\n\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:24:50.051994Z","iopub.execute_input":"2025-11-13T07:24:50.052928Z","iopub.status.idle":"2025-11-13T07:24:54.669883Z","shell.execute_reply.started":"2025-11-13T07:24:50.052901Z","shell.execute_reply":"2025-11-13T07:24:54.668942Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"model.eval()\nall_preds = []\ni = 0\nfor X in (pbar := tqdm(test_loader, desc='Test DataLoader')):\n    i += 1\n    X = {k: v.to(device) for k, v in X.items()}\n    with torch.no_grad():\n        logits = model(**X).logits\n    probs = torch.argmax(torch.softmax(logits, dim=-1), dim=-1)\n    all_preds.extend(probs.detach().cpu().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:24:54.671269Z","iopub.execute_input":"2025-11-13T07:24:54.671525Z","iopub.status.idle":"2025-11-13T07:26:31.730676Z","shell.execute_reply.started":"2025-11-13T07:24:54.671506Z","shell.execute_reply":"2025-11-13T07:26:31.729838Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Test DataLoader:   0%|          | 0/807 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c2be134b37c4e17b1cadb6cd0d7fceb"}},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"subm['target'] = list(map(idx2class.get, all_preds))\nsubm.to_csv(\"submission.csv\", index=False)\nsubm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:26:31.732010Z","iopub.execute_input":"2025-11-13T07:26:31.732298Z","iopub.status.idle":"2025-11-13T07:26:31.801490Z","shell.execute_reply.started":"2025-11-13T07:26:31.732279Z","shell.execute_reply":"2025-11-13T07:26:31.800881Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"       Unnamed: 0          target\n0               0        relevant\n1               1        relevant\n2               2   relevant_plus\n3               3     no_relevant\n4               4  relevant_minus\n...           ...             ...\n51631       51631  relevant_minus\n51632       51632        relevant\n51633       51633        relevant\n51634       51634  relevant_minus\n51635       51635        relevant\n\n[51636 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>relevant</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>relevant</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>relevant_plus</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>no_relevant</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>relevant_minus</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51631</th>\n      <td>51631</td>\n      <td>relevant_minus</td>\n    </tr>\n    <tr>\n      <th>51632</th>\n      <td>51632</td>\n      <td>relevant</td>\n    </tr>\n    <tr>\n      <th>51633</th>\n      <td>51633</td>\n      <td>relevant</td>\n    </tr>\n    <tr>\n      <th>51634</th>\n      <td>51634</td>\n      <td>relevant_minus</td>\n    </tr>\n    <tr>\n      <th>51635</th>\n      <td>51635</td>\n      <td>relevant</td>\n    </tr>\n  </tbody>\n</table>\n<p>51636 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"subm['target'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:26:31.802163Z","iopub.execute_input":"2025-11-13T07:26:31.802354Z","iopub.status.idle":"2025-11-13T07:26:31.810337Z","shell.execute_reply.started":"2025-11-13T07:26:31.802340Z","shell.execute_reply":"2025-11-13T07:26:31.809766Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"target\nrelevant_minus    19334\nrelevant          16579\nno_relevant        8842\nrelevant_plus      6881\nName: count, dtype: int64"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
